\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{italian}
\@writefile{toc}{\select@language{italian}}
\@writefile{lof}{\select@language{italian}}
\@writefile{lot}{\select@language{italian}}
\select@language{italian}
\@writefile{toc}{\select@language{italian}}
\@writefile{lof}{\select@language{italian}}
\@writefile{lot}{\select@language{italian}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Analisi dei cluster}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}clustering via K-means}{1}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Scelta degli attributi e della funzione distanza}{1}{subsubsection.1.1.1}}
\@writefile{toc}{\contentsline {paragraph}{}{1}{section*.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Identificazione del miglior valore di k}{1}{subsubsection.1.1.2}}
\newlabel{trovare_valore_di_k}{{1.1.2}{1}{Identificazione del miglior valore di k}{subsubsection.1.1.2}{}}
\@writefile{toc}{\contentsline {paragraph}{}{1}{section*.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Andamento della SSE e silhouette al variare di k. Il punto di gomito \IeC {\`e} scelto per $k=8$, cui corrisponde $SSE=1474$ e $silhouette=0.27$.\relax }}{1}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:sse_sil_vs_k}{{1}{1}{Andamento della SSE e silhouette al variare di k. Il punto di gomito è scelto per $k=8$, cui corrisponde $SSE=1474$ e $silhouette=0.27$.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {paragraph}{}{1}{section*.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}Caratterizzazione dei clusters ottenuti}{1}{subsubsection.1.1.3}}
\@writefile{toc}{\contentsline {paragraph}{}{2}{section*.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Analisi dei valori degli attributi dei centroidi ottenuti. Ogni centroide \IeC {\`e} rappresentato da un insieme di segmenti, i cui estremi marcano i valori degli attributi. Gli attributi dei clusters 1 e 3 seguono lo stesso andamento.\relax }}{2}{figure.caption.6}}
\newlabel{fig:centroids}{{2}{2}{Analisi dei valori degli attributi dei centroidi ottenuti. Ogni centroide è rappresentato da un insieme di segmenti, i cui estremi marcano i valori degli attributi. Gli attributi dei clusters 1 e 3 seguono lo stesso andamento.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {paragraph}{}{2}{section*.7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.4}Visualizzazione del clustering via Principal Component Analysis}{2}{subsubsection.1.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Clustering via DBSCAN}{2}{subsection.1.2}}
\newlabel{fig:kde_dataset}{{3a}{3}{Subfigure 3a}{subfigure.3.1}{}}
\newlabel{sub@fig:kde_dataset}{{(a)}{a}{Subfigure 3a\relax }{subfigure.3.1}{}}
\newlabel{fig:kde_4}{{3b}{3}{Subfigure 3b}{subfigure.3.2}{}}
\newlabel{sub@fig:kde_4}{{(b)}{b}{Subfigure 3b\relax }{subfigure.3.2}{}}
\newlabel{fig:kde_7}{{3c}{3}{Subfigure 3c}{subfigure.3.3}{}}
\newlabel{sub@fig:kde_7}{{(c)}{c}{Subfigure 3c\relax }{subfigure.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Kernel density estimation degli attributi \subref {fig:kde_dataset} per l'intero dataset, \subref {fig:kde_4} per il cluster 4, e \subref {fig:kde_7} per il cluster 7. Gli acuti picchi dell'attributo $satisfaction\_level$ per i cluster 4 e 7 non sono cos\IeC {\`\i } pronunciati anche nell'intero dataset.\relax }}{3}{figure.caption.8}}
\newlabel{fig:kde_clusters}{{3}{3}{Kernel density estimation degli attributi \protect \subref {fig:kde_dataset} per l'intero dataset, \protect \subref {fig:kde_4} per il cluster 4, e \protect \subref {fig:kde_7} per il cluster 7. Gli acuti picchi dell'attributo $satisfaction\_level$ per i cluster 4 e 7 non sono così pronunciati anche nell'intero dataset.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{3}{subfigure.3.1}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{3}{subfigure.3.2}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{3}{subfigure.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Visualizzazione del clustering in 2D. Ad ogni punto di dati \IeC {\`e} associata una label. Ogni label \IeC {\`e} rappresentata con un colore. Dataset denso a forma di ellisse. Un outlier \IeC {\`e} chiaramente presente in alto a destra.\relax }}{3}{figure.caption.9}}
\newlabel{fig:pca_kmeans}{{4}{3}{Visualizzazione del clustering in 2D. Ad ogni punto di dati è associata una label. Ogni label è rappresentata con un colore. Dataset denso a forma di ellisse. Un outlier è chiaramente presente in alto a destra.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Studio dei parametri minPoints ed epsilon}{3}{subsubsection.1.2.1}}
\@writefile{toc}{\contentsline {paragraph}{}{3}{section*.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Distanze tra ogni punto dati e il suo $k-th$ punto pi\IeC {\`u} vicino ordinate in maniera non-crescente.\relax }}{4}{figure.caption.10}}
\newlabel{fig:epsilon_selection}{{5}{4}{Distanze tra ogni punto dati e il suo $k-th$ punto più vicino ordinate in maniera non-crescente.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Andamento della silhouette per epsilon che varia nell'intervallo $\left [0.1,0.3\right ]$. Prima di $\epsilon =0.2$ la silhouette \IeC {\`e} negativa, mentre \IeC {\`e} presente un grande incremento per $\epsilon \cong  0.225$. Dopo $\epsilon \cong  0.26$ \IeC {\`e} presente un plateau. Si \IeC {\`e} scelto $\epsilon = 0.26$, ottenendo $silhouette=0.344$.\relax }}{4}{figure.caption.12}}
\newlabel{fig:epsilon_vs_silhouette}{{6}{4}{Andamento della silhouette per epsilon che varia nell'intervallo $\left [0.1,0.3\right ]$. Prima di $\epsilon =0.2$ la silhouette è negativa, mentre è presente un grande incremento per $\epsilon \cong 0.225$. Dopo $\epsilon \cong 0.26$ è presente un plateau. Si è scelto $\epsilon = 0.26$, ottenendo $silhouette=0.344$.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Caratterizzazione ed interpretazione dei clusters}{4}{subsubsection.1.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Clustering Gerarchico}{4}{subsection.1.3}}
\@writefile{toc}{\contentsline {paragraph}{Complete linkage.}{4}{section*.13}}
\@writefile{toc}{\contentsline {paragraph}{Average Linkage.}{4}{section*.15}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Dendogramma per il metodo complete linkage. La riga verticale nera rappresenta il taglio del dendogramma a distanza $1.85$ risultante in 2 clusters.\relax }}{5}{figure.caption.14}}
\newlabel{fig:complete_linkage}{{7}{5}{Dendogramma per il metodo complete linkage. La riga verticale nera rappresenta il taglio del dendogramma a distanza $1.85$ risultante in 2 clusters.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Dendogramma per il metodo average linkage. La riga che taglia il dendogramma a distanza 1 risulta in 4 clusters.\relax }}{5}{figure.caption.16}}
\newlabel{fig:average_linkage}{{8}{5}{Dendogramma per il metodo average linkage. La riga che taglia il dendogramma a distanza 1 risulta in 4 clusters.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {paragraph}{Ward's method Linkage.}{5}{section*.17}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Dendogramma per il metodo Ward. Il taglio a distanza 15 fa ottenere 7 clusters.\relax }}{5}{figure.caption.18}}
\newlabel{fig:ward_linkage}{{9}{5}{Dendogramma per il metodo Ward. Il taglio a distanza 15 fa ottenere 7 clusters.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {paragraph}{Calcolo della silhouette.}{5}{section*.19}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Intervallo di valori assunti dalla silhouette, per un numero di cluster C da 2 a 9, per i metodi complete, average e Ward. Risultati ottenuti con un algoritmo non strutturato (U) e strutturato considerando n=100 vicini. Per ogni metodo il miglior risultato \IeC {\`e} mostrato in grassetto. I numeri sottolineati sono risultati altrettanto buoni.\relax }}{6}{table.caption.20}}
\newlabel{tab:hierarchical_silhouette}{{1}{6}{Intervallo di valori assunti dalla silhouette, per un numero di cluster C da 2 a 9, per i metodi complete, average e Ward. Risultati ottenuti con un algoritmo non strutturato (U) e strutturato considerando n=100 vicini. Per ogni metodo il miglior risultato è mostrato in grassetto. I numeri sottolineati sono risultati altrettanto buoni.\relax }{table.caption.20}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Confronto delle prestazioni degli algoritmi di clustering K-means, DBSCAN e gerarchico. Risultati ottenuti impostando i seguenti parametri: K=8 per K-means. MinPoints=4, $\epsilon =0.26$ per DBSCAN ottenendo 2 clusters. Metodo average linkage con n=100 vicini per clustering gerarchico, ottenendo 2 clusters. In grassetto il miglior risultato.\relax }}{6}{table.caption.21}}
\newlabel{tab:performance_clustering}{{2}{6}{Confronto delle prestazioni degli algoritmi di clustering K-means, DBSCAN e gerarchico. Risultati ottenuti impostando i seguenti parametri: K=8 per K-means. MinPoints=4, $\epsilon =0.26$ per DBSCAN ottenendo 2 clusters. Metodo average linkage con n=100 vicini per clustering gerarchico, ottenendo 2 clusters. In grassetto il miglior risultato.\relax }{table.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Valutazione del miglior metodo di clustering}{6}{subsection.1.4}}
